{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "140c350c",
   "metadata": {},
   "source": [
    "## Regression model with PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad19ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eefeffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cdb333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = os.path.join(os.getcwd(), 'forestfires.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56081cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e098541",
   "metadata": {},
   "source": [
    "### Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf68cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1: not scaled or transformed with area as a target variable. \n",
    "# area is very skewed towards 0, it can have a bad impact on the result.\n",
    "#     features: ['FFMC','temp','RH','wind','rain', 'ISI']\n",
    "#     target: 'area'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f6fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pytoch dataset\n",
    "batch_size = 20\n",
    "\n",
    "def get_dl(inputs, targets, batch_size):\n",
    "    dataset = TensorDataset(torch.tensor(inputs, dtype = torch.float32), torch.tensor(targets, dtype = torch.float32))\n",
    "\n",
    "    # we'll split data into train and test using random split \n",
    "    train, val = random_split(dataset, [450, 67])\n",
    "\n",
    "    # create data loaders\n",
    "    train_dl = DataLoader(train, batch_size = batch_size, shuffle = True)\n",
    "    val_dl = DataLoader(val, batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "    return train_dl, val_dl\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c4aa6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 data\n",
    "inputs1 = df[['FFMC','temp','RH','wind','rain', 'ISI']].values\n",
    "target1 = df['area'].values\n",
    "\n",
    "train_dl1, val_dl1 = get_dl(inputs1, target1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "479f6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output sizes are the same for all models. input = 1 and putput = 6.\n",
    "output_size = 1\n",
    "input_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "019fc016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple linear model\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        output = self.linear(x)\n",
    "        return output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad7f9f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, train_dl, epochs):\n",
    "    history = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        print(f'Epoch # %s' %e)\n",
    "        losses = 0\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            print(f'    Batch # %s' %i)\n",
    "\n",
    "            out = model(inputs).squeeze() #predictions\n",
    "            loss = F.l1_loss(out, targets) # loss\n",
    "            loss.backward()\n",
    "            print('Loss: %s' %loss)\n",
    "            optimizer.step()\n",
    "            losses += loss\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        history.append(losses/len(train_dl))\n",
    "    return history\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d6a9673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(model, val_dl):\n",
    "    \n",
    "    losses = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(val_dl):\n",
    "            out = model(inputs).squeeze() #predictions\n",
    "            loss = F.l1_loss(out, targets) # loss\n",
    "            losses += loss\n",
    "    val_loss = losses/len(val_dl)\n",
    "    \n",
    "    return val_loss\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da2f066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig, axes = plt.subplots(figsize = (10, 6))\n",
    "    plt.plot(history, '-x')\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('train_loss')\n",
    "    plt.title('Training loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfebdaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0\n",
      "    Batch # 0\n",
      "Loss: tensor(15.6558, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(63.7969, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(4.6892, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(7.8535, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(2.8863, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(45.7259, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(7.9071, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.9170, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(9.0813, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(6.3397, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(18.1396, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(15.2462, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(29.3667, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(17.0233, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(3.5241, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(11.8186, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(12.1877, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.5403, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.6572, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(10.2444, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(3.8661, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(12.2630, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(2.4264, grad_fn=<L1LossBackward>)\n",
      "Epoch # 1\n",
      "    Batch # 0\n",
      "Loss: tensor(13.2497, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.7385, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(12.8426, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(10.8748, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(22.7088, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(10.2522, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(51.0714, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(4.3390, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(15.3587, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.8732, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(4.9582, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(8.7365, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(9.5706, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(2.5746, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(7.6298, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(63.5064, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(4.3958, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(9.6204, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(23.3353, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(5.4374, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(15.2292, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(3.7017, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(12.1131, grad_fn=<L1LossBackward>)\n",
      "Epoch # 2\n",
      "    Batch # 0\n",
      "Loss: tensor(8.4311, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.9368, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(10.9356, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(7.2630, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(9.5934, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(7.2117, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(17.9265, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.2230, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(20.3229, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(7.9653, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(9.9219, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(12.0184, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(12.4253, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(6.6329, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(60.7960, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(10.4784, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(1.8435, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.2109, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(8.1210, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(16.5979, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(11.3712, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(55.1548, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(7.8018, grad_fn=<L1LossBackward>)\n",
      "Epoch # 3\n",
      "    Batch # 0\n",
      "Loss: tensor(5.3182, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(8.7682, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(13.8549, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(7.1220, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(6.4019, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(7.2276, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(7.2993, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(45.0050, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(15.5292, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(8.3585, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(10.9022, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(12.0441, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(9.4292, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(7.7234, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.9354, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(8.6095, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(94.7465, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.1624, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(8.4805, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(9.9933, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(8.0770, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(18.1656, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(9.6162, grad_fn=<L1LossBackward>)\n",
      "Epoch # 4\n",
      "    Batch # 0\n",
      "Loss: tensor(8.3557, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.4557, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(6.4532, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(10.8609, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(9.5241, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(69.3740, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(9.1694, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(44.5353, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(10.5247, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(14.1137, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(3.2854, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(12.9630, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(7.5770, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(21.1297, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(18.9187, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(8.0354, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(6.1769, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(18.8036, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.0050, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(8.9693, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(15.1623, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(3.8488, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(4.4362, grad_fn=<L1LossBackward>)\n",
      "Epoch # 5\n",
      "    Batch # 0\n",
      "Loss: tensor(18.8369, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(4.8209, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(21.5128, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(17.0612, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(4.8583, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.8092, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(7.1884, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(44.8881, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(7.7956, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(11.1275, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(9.7636, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(7.3802, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(69.4016, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(8.3572, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(7.2862, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(7.3828, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(19.0476, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(15.7088, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(1.6904, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(8.2019, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(3.7721, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(10.4998, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(5.6451, grad_fn=<L1LossBackward>)\n",
      "Epoch # 6\n",
      "    Batch # 0\n",
      "Loss: tensor(10.1687, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(21.9424, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(41.4046, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(7.7384, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(7.8966, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(9.9179, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.9731, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(7.3622, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(12.7546, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(6.2836, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(16.1077, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(7.6830, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(6.3005, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(3.3626, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(64.1355, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(9.7486, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(17.9785, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(29.1544, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(7.2923, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(2.7483, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.5847, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(11.6394, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(6.5650, grad_fn=<L1LossBackward>)\n",
      "Epoch # 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch # 0\n",
      "Loss: tensor(6.9313, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(22.6806, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(60.6938, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(7.7936, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(9.1707, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(2.2898, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(8.4910, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(8.0107, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(15.1598, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(2.2316, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(12.3715, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(9.1919, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(12.1998, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(12.3964, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(17.8177, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.5211, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(9.7597, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(7.5780, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(5.1104, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(26.9112, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.7594, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(42.4033, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(2.4858, grad_fn=<L1LossBackward>)\n",
      "Epoch # 8\n",
      "    Batch # 0\n",
      "Loss: tensor(6.5915, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(12.7131, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(7.2706, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(4.9860, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(14.2860, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(7.7929, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(8.8494, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(21.0300, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(63.1951, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.0581, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(14.2087, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(22.1463, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(50.1607, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(7.8492, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.7737, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(8.6427, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(10.0179, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(4.9653, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(8.3891, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(5.8543, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(25.5975, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(3.7235, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(5.6754, grad_fn=<L1LossBackward>)\n",
      "Epoch # 9\n",
      "    Batch # 0\n",
      "Loss: tensor(62.6735, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.9709, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(7.3264, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(7.2643, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(9.6936, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(3.3560, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(15.4433, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.9161, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(2.0686, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(23.8999, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(5.7743, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(2.6291, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(16.7764, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(6.3541, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(6.7193, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(14.3428, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(35.3312, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(9.6459, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(10.5649, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(5.8202, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(41.6929, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(11.1225, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(6.8216, grad_fn=<L1LossBackward>)\n",
      "Epoch # 10\n",
      "    Batch # 0\n",
      "Loss: tensor(14.2294, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(7.7719, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(6.0737, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(14.3931, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(14.9405, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(16.6965, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(14.4962, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(8.1760, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(8.6700, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(55.7081, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(8.4617, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(8.4499, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(2.9412, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(5.8148, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(19.8975, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(62.4938, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(4.0305, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.3862, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(9.2565, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(9.7604, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(3.1965, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(8.5938, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(11.3424, grad_fn=<L1LossBackward>)\n",
      "Epoch # 11\n",
      "    Batch # 0\n",
      "Loss: tensor(15.5299, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(9.8005, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(6.3396, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(3.3768, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(4.8178, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(43.5775, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(19.0252, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(10.3943, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(3.4556, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(3.5289, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(7.6167, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(4.8437, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(19.8141, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(18.3470, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(3.4198, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(24.1299, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(9.8772, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(3.7192, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(16.2187, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(9.5390, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(56.6175, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(5.5086, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(20.3706, grad_fn=<L1LossBackward>)\n",
      "Epoch # 12\n",
      "    Batch # 0\n",
      "Loss: tensor(16.6167, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.4219, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(9.5478, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(13.9093, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(57.5773, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(16.4721, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(11.3714, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.6012, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(3.7855, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(3.4454, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(13.7876, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.3250, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(12.4801, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(41.7719, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(11.3052, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.9418, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(7.6099, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(4.3801, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(9.2000, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(16.3165, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(20.4518, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(13.5377, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(8.5249, grad_fn=<L1LossBackward>)\n",
      "Epoch # 13\n",
      "    Batch # 0\n",
      "Loss: tensor(6.0900, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(10.1964, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(14.5746, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(6.9417, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(12.7474, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(52.6563, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(7.1057, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.7519, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(7.9461, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(7.3159, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(4.5983, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(8.2311, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(1.1846, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(10.5631, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(21.1224, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(8.0946, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(17.2921, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(10.4867, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(14.2297, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(18.3345, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.9631, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(6.2628, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(117.4222, grad_fn=<L1LossBackward>)\n",
      "Epoch # 14\n",
      "    Batch # 0\n",
      "Loss: tensor(8.1255, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(14.4211, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(8.1009, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(18.3648, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(6.3573, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(17.4302, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(6.9032, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(42.6823, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(10.3673, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(7.5855, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(4.5979, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.9793, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(10.3676, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(16.3889, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(8.5003, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(16.0579, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(19.4634, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(5.8668, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(7.1170, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(60.8661, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(7.6094, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(8.2271, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(11.7542, grad_fn=<L1LossBackward>)\n",
      "Epoch # 15\n",
      "    Batch # 0\n",
      "Loss: tensor(8.0989, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(4.5871, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(6.9400, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.7535, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(14.3798, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(8.1192, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(8.7675, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(17.4217, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(8.9403, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(8.5979, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(10.1561, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(7.5582, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(7.8886, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(17.1492, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(16.8703, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(4.4264, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(9.8049, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(9.3189, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(45.8911, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(15.3259, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.8310, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(73.6622, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(7.1698, grad_fn=<L1LossBackward>)\n",
      "Epoch # 16\n",
      "    Batch # 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(4.3783, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(15.5325, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(11.6423, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(64.8884, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(6.7861, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(4.7175, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(18.5740, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(9.6656, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.8697, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(8.6899, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(5.6296, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.2938, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(8.9388, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(19.2070, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(6.3411, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(51.0948, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(8.6514, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.7266, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(11.7187, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(19.8716, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(3.6622, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(20.7968, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(4.3090, grad_fn=<L1LossBackward>)\n",
      "Epoch # 17\n",
      "    Batch # 0\n",
      "Loss: tensor(27.9104, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.2389, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(10.3510, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(12.1871, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(5.4279, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(9.3622, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(8.3668, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(11.6626, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(12.7384, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(67.0688, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(8.0369, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(19.8441, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(4.3140, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(5.8841, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(3.1573, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(7.4939, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(6.2593, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(7.5443, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(54.5903, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(6.0087, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(10.0692, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(12.1155, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(9.9935, grad_fn=<L1LossBackward>)\n",
      "Epoch # 18\n",
      "    Batch # 0\n",
      "Loss: tensor(62.0118, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(19.0916, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(17.7817, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(10.5988, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(4.9159, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(7.7019, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(51.1885, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(12.7054, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(3.1518, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(9.9792, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(13.4069, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(7.6738, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(6.1693, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(9.5755, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(24.2590, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(10.1634, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(4.3844, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(3.7234, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(8.8619, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(3.3426, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(9.2181, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(14.6976, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(6.6740, grad_fn=<L1LossBackward>)\n",
      "Epoch # 19\n",
      "    Batch # 0\n",
      "Loss: tensor(61.9938, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(17.5043, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(7.4366, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(13.6003, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(7.7412, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(14.4565, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(11.3931, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(12.6340, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(7.1455, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(12.4172, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(11.1914, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(20.2397, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(6.9045, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(43.3251, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(15.5173, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(7.9934, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(4.9791, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(2.9875, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(6.2334, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(13.1365, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(8.5444, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(7.3408, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(9.7083, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model1 = Linear()\n",
    "epochs = 20\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr= 0.001)\n",
    "history1 = train_model(model1, optimizer, train_dl1, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b311a081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b1dea81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0\n",
      "    Batch # 0\n",
      "Loss: tensor(12.7704, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(32.0438, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(17.9724, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(22.6676, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(8.5967, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.2459, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(8.8988, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(9.5624, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(10.4426, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(9.7159, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(4.2261, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(7.2054, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(4.3196, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(6.3193, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(6.5921, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(64.9122, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(8.5576, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(14.4863, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(7.5632, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.8625, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(51.6221, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(5.8517, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(31.1374, grad_fn=<L1LossBackward>)\n",
      "Epoch # 1\n",
      "    Batch # 0\n",
      "Loss: tensor(5.7304, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.6303, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.0107, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(4.6627, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(15.4110, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(11.1192, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(16.1165, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(19.8640, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(11.4281, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(7.7818, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(10.3958, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(44.7557, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(23.1080, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(8.9021, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.4013, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(19.8569, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(60.9714, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.4575, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.8029, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(6.9605, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(3.8529, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(6.0652, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(3.3165, grad_fn=<L1LossBackward>)\n",
      "Epoch # 2\n",
      "    Batch # 0\n",
      "Loss: tensor(9.6528, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(9.2484, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(3.5601, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(41.3468, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(3.3916, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(3.4453, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.6107, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(60.8685, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(7.5791, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(20.4188, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(15.7758, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(12.5654, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.7919, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(12.2139, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.2787, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.5037, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(21.2890, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.7041, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(3.5859, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(15.0111, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(5.9328, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(18.1133, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(23.2998, grad_fn=<L1LossBackward>)\n",
      "Epoch # 3\n",
      "    Batch # 0\n",
      "Loss: tensor(22.6971, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.1178, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(7.3430, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(9.4676, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(22.1580, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(9.2970, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(45.9195, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(7.0292, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(26.1669, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(3.9553, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(63.3785, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.7323, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(9.9864, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(7.8656, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(11.9669, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(13.0054, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(5.4584, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(3.1731, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(5.6389, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(2.2011, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.8505, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(5.8102, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(2.1291, grad_fn=<L1LossBackward>)\n",
      "Epoch # 4\n",
      "    Batch # 0\n",
      "Loss: tensor(5.4760, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(17.1109, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(46.1668, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(4.2419, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(7.8071, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.6005, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(13.8799, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(7.6583, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(8.2421, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(7.1629, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(2.8945, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(7.6526, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(7.7946, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(59.4316, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(18.8823, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.9144, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(6.5751, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(14.0811, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.2510, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(9.7889, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(7.5772, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(22.9713, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(17.2790, grad_fn=<L1LossBackward>)\n",
      "Epoch # 5\n",
      "    Batch # 0\n",
      "Loss: tensor(6.8212, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(4.1813, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(2.4295, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(10.8858, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(10.2108, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(57.6402, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(14.0926, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(11.3469, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(13.4263, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(3.3815, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(58.6902, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(7.0728, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(6.5303, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(6.7330, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.9631, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(8.3398, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(9.9800, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(4.6811, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(5.2880, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(9.1328, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(13.4264, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(25.9084, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(10.7347, grad_fn=<L1LossBackward>)\n",
      "Epoch # 6\n",
      "    Batch # 0\n",
      "Loss: tensor(4.7727, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(10.4087, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(3.6591, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(3.8184, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(7.1907, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(2.5370, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(16.3235, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(9.5376, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(6.4489, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.8072, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(58.6980, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(10.0141, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(15.0343, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(25.4585, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(13.1548, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.5034, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(9.9065, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(13.9275, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(5.1880, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(16.1144, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(40.1741, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(11.2755, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(6.8745, grad_fn=<L1LossBackward>)\n",
      "Epoch # 7\n",
      "    Batch # 0\n",
      "Loss: tensor(5.8537, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(61.9989, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(9.8219, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(8.1795, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(9.0506, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(2.7085, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(16.6483, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(4.1032, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(8.7186, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(40.2819, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(6.2004, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(4.6288, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(6.1399, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(5.3027, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(16.9163, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(14.6431, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(7.0392, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(13.8107, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(21.0827, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(13.8154, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.5894, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(14.0082, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(3.2346, grad_fn=<L1LossBackward>)\n",
      "Epoch # 8\n",
      "    Batch # 0\n",
      "Loss: tensor(11.1749, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(15.9589, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.2396, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.1037, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(16.9868, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(14.5276, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(4.0528, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(3.9165, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(47.1982, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(10.9941, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(4.4922, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(8.6663, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(9.2001, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(2.8519, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.1972, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(8.5504, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(4.6827, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(16.9631, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(57.7417, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.9187, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(15.8150, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(21.2956, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(5.3170, grad_fn=<L1LossBackward>)\n",
      "Epoch # 9\n",
      "    Batch # 0\n",
      "Loss: tensor(11.3010, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(4.9537, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.9897, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.7660, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(8.8293, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.9576, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(57.6940, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(4.7249, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.8480, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(10.1595, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(22.3439, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(5.8119, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(10.5013, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(4.0762, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(15.2437, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.0818, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(24.6474, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(9.9853, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.1800, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(14.7218, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(2.7951, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(53.4397, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(4.7210, grad_fn=<L1LossBackward>)\n",
      "Epoch # 10\n",
      "    Batch # 0\n",
      "Loss: tensor(6.6165, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.5714, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(3.5214, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(19.8575, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(11.3177, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(5.1686, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(7.4100, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(4.3343, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(9.2229, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(7.3293, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(7.8473, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(15.6563, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(2.0586, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(5.7469, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(19.3848, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(4.2993, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(10.0879, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(60.6980, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(44.7304, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.7771, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(8.0700, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(16.9272, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(30.4277, grad_fn=<L1LossBackward>)\n",
      "Epoch # 11\n",
      "    Batch # 0\n",
      "Loss: tensor(3.4804, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(16.7007, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(16.4483, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(3.0801, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(7.7675, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(3.6381, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(11.2482, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(2.2854, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(11.2782, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(3.4296, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(3.5205, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(71.8779, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(6.7538, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(11.3332, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(8.0382, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(8.4802, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(9.2023, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(5.3339, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.2271, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(11.9492, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.4943, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(14.3437, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(110.4878, grad_fn=<L1LossBackward>)\n",
      "Epoch # 12\n",
      "    Batch # 0\n",
      "Loss: tensor(13.7343, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(42.8178, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(4.9406, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(12.1996, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(59.8291, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.2217, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.3917, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(7.4258, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(15.2122, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(11.6345, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(6.0186, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(17.9107, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(12.1211, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(11.2458, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.7109, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(14.6071, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(3.8263, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(16.3984, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.8972, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.3229, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(12.3363, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(6.0356, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(4.9075, grad_fn=<L1LossBackward>)\n",
      "Epoch # 13\n",
      "    Batch # 0\n",
      "Loss: tensor(12.4095, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.3539, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(8.3521, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.5559, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(3.7637, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(9.4364, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.6169, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(10.8393, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(7.6616, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(3.3543, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(17.6951, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(4.9277, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(21.5148, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(62.1337, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.0689, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(16.7250, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(44.0073, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(14.6794, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(5.9983, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.7243, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.1013, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(16.5809, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(6.9420, grad_fn=<L1LossBackward>)\n",
      "Epoch # 14\n",
      "    Batch # 0\n",
      "Loss: tensor(52.3275, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(11.5249, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.5689, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(12.2393, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(7.2026, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(4.0964, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(2.9408, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(16.1340, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(6.0758, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(6.4869, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(8.4131, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(12.9499, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(7.6758, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(6.6073, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(26.0741, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(4.7313, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(7.0885, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(7.8818, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(7.7261, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(8.0996, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(9.9173, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(6.6474, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(114.3047, grad_fn=<L1LossBackward>)\n",
      "Epoch # 15\n",
      "    Batch # 0\n",
      "Loss: tensor(3.9345, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(7.0032, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(7.8035, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(8.0059, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(21.9539, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.8787, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(9.3311, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(13.9909, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(62.7024, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(9.2969, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(5.3956, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(4.3173, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(14.6640, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(44.1466, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.4279, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(3.6225, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(25.2427, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(4.1641, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(3.7122, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(14.8472, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(7.4654, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(3.3518, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(17.7203, grad_fn=<L1LossBackward>)\n",
      "Epoch # 16\n",
      "    Batch # 0\n",
      "Loss: tensor(3.9820, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(13.3407, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(10.3018, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(12.9978, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(15.9190, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(4.7723, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(12.3080, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(2.5572, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.8663, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(6.6343, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(8.4672, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(4.7746, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(6.5456, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(59.8472, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(7.2423, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(7.2420, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(10.8030, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(24.3059, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(10.9553, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.4204, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(44.4788, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(16.5681, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(1.6400, grad_fn=<L1LossBackward>)\n",
      "Epoch # 17\n",
      "    Batch # 0\n",
      "Loss: tensor(4.5939, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.3749, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(42.8816, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(7.4830, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(5.6903, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(3.9690, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(3.9802, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(8.4037, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(10.1495, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(2.6315, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(14.7286, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(12.5514, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(12.8705, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(7.6819, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(12.7206, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(9.4459, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(8.3855, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(16.8713, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(12.7196, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(11.4692, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(13.0998, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(64.7112, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(3.7478, grad_fn=<L1LossBackward>)\n",
      "Epoch # 18\n",
      "    Batch # 0\n",
      "Loss: tensor(6.8987, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(8.6887, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(7.1722, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.5174, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(6.9719, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(10.0573, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(4.0898, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(27.8916, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(58.5969, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(6.2594, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(3.9292, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.2980, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(26.9978, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(6.0595, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(40.3267, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(14.0112, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(6.8215, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(4.3127, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(5.7821, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(14.4389, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.3865, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(11.9335, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(10.8638, grad_fn=<L1LossBackward>)\n",
      "Epoch # 19\n",
      "    Batch # 0\n",
      "Loss: tensor(10.3143, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(3.6533, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(10.0645, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(61.4532, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(15.0091, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(5.8894, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(6.2775, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(17.1666, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.1752, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(10.1988, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(12.9483, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(7.4116, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(9.6177, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(3.5023, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(51.3871, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(13.5095, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(3.3965, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.9903, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.6913, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(3.3439, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(22.8021, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(6.4721, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(7.3192, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model1 = Linear()\n",
    "epochs = 20\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr= 0.0001)\n",
    "history1 = train_model(model1, optimizer, train_dl1, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30cdbb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5123d4",
   "metadata": {},
   "source": [
    "This model uses all unscaled variables and a traget too skewed to 0. Training loss per epoch goes down but has many peaks. The model finds it hard to converge. The same demonstrates the evaluation scores. The smaller learning rate performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08148ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f677b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(x):\n",
    "    return np.log(x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f2d70df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2 data\n",
    "# We log transform are to get more homogenious values\n",
    "df['area_log'] = df['area'].apply(lambda x: log_transform(x))\n",
    "\n",
    "inputs2 = df[['FFMC','temp','RH','wind','rain', 'ISI']].values\n",
    "target2 = df['area_log'].values\n",
    "\n",
    "train_dl2, val_dl2 = get_dl(inputs2, target2, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cf5da05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0\n",
      "    Batch # 0\n",
      "Loss: tensor(39.0785, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(27.8639, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(17.1874, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.2217, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(4.1168, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.5426, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(4.1158, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(6.0724, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(4.1447, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(6.6669, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(3.9773, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.6627, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(3.1117, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(4.0237, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.9021, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(4.0761, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(5.3973, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(5.1629, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(6.0174, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(5.6956, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(5.2703, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(7.0438, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(3.2221, grad_fn=<L1LossBackward>)\n",
      "Epoch # 1\n",
      "    Batch # 0\n",
      "Loss: tensor(5.6679, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.3497, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.8987, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.4903, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(5.4712, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.2653, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.2658, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(6.1785, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.1801, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.7081, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(5.0398, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.0596, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(4.9273, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(5.9080, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(3.8032, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.3208, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(5.2300, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(5.3089, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.8401, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.4075, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.5249, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(5.0788, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(6.2085, grad_fn=<L1LossBackward>)\n",
      "Epoch # 2\n",
      "    Batch # 0\n",
      "Loss: tensor(5.0386, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.8255, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(4.4237, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.1353, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(5.9991, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(4.9256, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.7912, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.0705, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(4.3866, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.9417, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(4.6817, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(5.1920, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(6.1242, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(5.2188, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.7872, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.3574, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(4.9369, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(5.5684, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.8721, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.7436, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.1680, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(6.4346, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(3.5945, grad_fn=<L1LossBackward>)\n",
      "Epoch # 3\n",
      "    Batch # 0\n",
      "Loss: tensor(7.6851, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(3.8730, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(7.2672, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(3.6020, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(6.3306, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(4.0882, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.7606, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.0209, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.3556, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.1050, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(6.2807, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(5.2189, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.7689, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(4.4319, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(7.6000, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.3798, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(5.1932, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(5.7445, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(5.7154, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(6.3313, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(5.4243, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(4.9420, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(4.5370, grad_fn=<L1LossBackward>)\n",
      "Epoch # 4\n",
      "    Batch # 0\n",
      "Loss: tensor(5.3388, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.0835, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(3.3769, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(6.7289, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(4.0069, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.9986, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(3.5881, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(6.2837, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.9975, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(4.9211, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(6.6371, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(4.5554, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(6.3719, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(3.6909, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(3.7686, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.8581, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(3.8410, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.8185, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(3.3641, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.6350, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.8398, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(4.9653, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(4.7365, grad_fn=<L1LossBackward>)\n",
      "Epoch # 5\n",
      "    Batch # 0\n",
      "Loss: tensor(5.6274, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.3071, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.1253, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(6.7349, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(3.9122, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.4027, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(4.1614, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(6.1005, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(4.6604, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(4.9874, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(4.3410, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.4654, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(4.0889, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(7.1868, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.1916, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.2269, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(4.4342, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(5.9754, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.6289, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(5.6528, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(3.9577, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(7.4323, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(2.9810, grad_fn=<L1LossBackward>)\n",
      "Epoch # 6\n",
      "    Batch # 0\n",
      "Loss: tensor(6.5226, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(3.7806, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(6.4335, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.5772, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(5.3184, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(4.4874, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.2868, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(6.1388, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.0231, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(4.6631, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(4.9060, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(5.8448, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(4.2308, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(6.3360, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.9640, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(4.8814, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(5.1256, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.2674, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.9755, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(5.3472, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.3062, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(4.1850, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(5.2757, grad_fn=<L1LossBackward>)\n",
      "Epoch # 7\n",
      "    Batch # 0\n",
      "Loss: tensor(5.0104, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(4.9049, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(4.5363, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(6.7720, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(3.9898, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(5.1396, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(4.6649, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(7.0491, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(3.0418, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(6.0598, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(4.7151, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(5.7602, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(3.8935, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(6.4409, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.6331, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.9694, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(4.1028, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(7.0955, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.1778, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(6.1390, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(3.7771, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(5.1087, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(5.3849, grad_fn=<L1LossBackward>)\n",
      "Epoch # 8\n",
      "    Batch # 0\n",
      "Loss: tensor(4.6883, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.2363, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(6.9549, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(4.3933, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(6.2192, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(4.1530, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.7286, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.8732, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(4.9240, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(6.0134, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(5.7558, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(5.0558, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(4.4739, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(5.0002, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.7935, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(4.5935, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(5.7671, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(4.5087, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(6.2011, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.3999, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.8372, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(4.4640, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(5.3914, grad_fn=<L1LossBackward>)\n",
      "Epoch # 9\n",
      "    Batch # 0\n",
      "Loss: tensor(4.7700, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.3132, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(6.2242, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(6.1806, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(4.6645, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(5.9018, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.7189, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.3769, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(3.7846, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(6.2841, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(3.3247, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(4.6482, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.8558, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(4.2699, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(6.1311, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.2391, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(5.2245, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(4.4041, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(6.2246, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.6207, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.9398, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(4.6187, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(4.3799, grad_fn=<L1LossBackward>)\n",
      "Epoch # 10\n",
      "    Batch # 0\n",
      "Loss: tensor(3.9777, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.3633, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.3698, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(6.9741, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(4.3536, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.4114, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(4.4777, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(6.1616, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(4.5152, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.9450, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(4.9593, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.6488, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(4.7636, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(6.8828, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.3533, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.4969, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(4.3061, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.6735, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.1432, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(5.9433, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.9616, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(5.0886, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(5.8347, grad_fn=<L1LossBackward>)\n",
      "Epoch # 11\n",
      "    Batch # 0\n",
      "Loss: tensor(4.8098, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.3910, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(4.7406, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.7866, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(4.0323, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.8658, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(3.9177, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(6.7854, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(3.4300, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(4.9954, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(7.0067, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(5.1850, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(6.2281, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(4.9966, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.5031, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(4.4027, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(6.7560, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(4.8870, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(6.4276, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.0497, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.5417, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(3.9139, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(6.0645, grad_fn=<L1LossBackward>)\n",
      "Epoch # 12\n",
      "    Batch # 0\n",
      "Loss: tensor(3.7316, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(4.0344, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(6.4775, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(4.9143, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(5.7331, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(3.9074, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(6.6537, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(3.9672, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(6.3221, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(4.7827, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(6.1896, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(4.5259, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(6.3067, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(4.4108, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.4005, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.2300, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(4.5742, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.3926, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.8039, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(6.4418, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.7055, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(5.8826, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(5.6523, grad_fn=<L1LossBackward>)\n",
      "Epoch # 13\n",
      "    Batch # 0\n",
      "Loss: tensor(5.6025, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(4.9599, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.8349, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(4.6821, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(5.3373, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.0600, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(4.9334, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.8142, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(4.4381, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.4594, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(5.2559, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(5.6895, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.9969, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(5.2401, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.5470, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.4528, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(5.0319, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(5.9285, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.3207, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.8361, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.3498, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(4.1468, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(4.9123, grad_fn=<L1LossBackward>)\n",
      "Epoch # 14\n",
      "    Batch # 0\n",
      "Loss: tensor(5.4878, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(4.2217, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(4.5951, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(6.9641, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(4.6113, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.0105, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(4.2497, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.9537, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(4.1314, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.3876, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(4.6470, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(5.2032, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.0143, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(4.3544, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(6.8333, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(4.0986, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(7.1521, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(3.7697, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(6.3191, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(5.0390, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.5810, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(2.7147, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(5.9195, grad_fn=<L1LossBackward>)\n",
      "Epoch # 15\n",
      "    Batch # 0\n",
      "Loss: tensor(3.3820, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.6994, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(3.8711, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(6.7723, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(3.7589, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.8663, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(3.4303, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(6.5603, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(3.7367, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(7.0408, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(2.8267, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.5207, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(4.4856, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(6.6115, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.1360, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.2997, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(4.3621, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.4623, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(3.7141, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(6.5137, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(3.8136, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(6.4375, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(3.7475, grad_fn=<L1LossBackward>)\n",
      "Epoch # 16\n",
      "    Batch # 0\n",
      "Loss: tensor(6.2859, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(3.5341, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.4870, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.5362, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(5.6922, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(4.9283, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.7092, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.0503, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(6.0957, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(4.1696, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(6.9314, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(4.5206, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.7420, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(4.9590, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(6.4050, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(3.8478, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(6.2229, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(4.1753, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(6.1098, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.4332, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.1282, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(3.8793, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(7.2888, grad_fn=<L1LossBackward>)\n",
      "Epoch # 17\n",
      "    Batch # 0\n",
      "Loss: tensor(3.6460, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.0885, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.4241, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(6.4881, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(4.0015, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.1980, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(4.5605, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.8831, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.4338, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(4.8116, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(5.3135, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(5.6216, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.6993, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(4.7000, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.3517, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.2529, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(5.6446, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(5.5092, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(5.3016, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(5.7693, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(5.4094, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(5.4605, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(5.5582, grad_fn=<L1LossBackward>)\n",
      "Epoch # 18\n",
      "    Batch # 0\n",
      "Loss: tensor(4.4729, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.2440, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.3249, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.4497, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(4.9154, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(4.8445, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(6.0891, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.3563, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.3519, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.1512, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(5.5673, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(4.5260, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.6433, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(4.7940, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.9411, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.0798, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(5.5124, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(4.9679, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(5.2227, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(5.1433, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(5.2054, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(5.6961, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(5.8027, grad_fn=<L1LossBackward>)\n",
      "Epoch # 19\n",
      "    Batch # 0\n",
      "Loss: tensor(5.1051, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.2196, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(4.7781, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.2674, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(5.3953, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(5.9077, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.2921, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.8916, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(4.8892, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.7704, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(5.3691, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.5039, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(4.6910, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(6.6874, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.6368, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.7192, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(4.1113, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.6664, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.4718, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(6.2339, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.6824, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(6.4202, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(5.0895, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model2 = Linear()\n",
    "epochs = 20\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr= 0.001)\n",
    "history2 = train_model(model2, optimizer, train_dl2, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83924a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e5ac18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on training: 1.8832\n",
      "Evaluate on validation : 1.7936\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate on training: {:.4f}'.format(val_model(model1, train_dl2)))\n",
    "print('Evaluate on validation : {:.4f}'.format(val_model(model1, val_dl2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e187779",
   "metadata": {},
   "source": [
    "Second model uses as a target area_log, that is a log transformed area. This may help to get better results. What we see is that the loss drops significantly and then stays on the same levels having peaks up an down.\n",
    "Transforming the target helped a model to converge faster though still at certain level it is hard for the model to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b1374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ea5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ce126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
